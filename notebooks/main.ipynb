{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit7902d3b6b96f4e0481d11ecc1823e43f",
   "display_name": "Python 3.7.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CascadeLSTM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append('../code/')\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from model import DeepTreeLSTMClassifier, DeepTreeLSTMRegressor\n",
    "from dataset import CascadeData\n",
    "from trainer import DeepTreeTrainer\n",
    "from callbacks import EarlyStopping, ExperimentLogger\n",
    "from itertools import product"
   ]
  },
  {
   "source": [
    "## CascadeLSTM Regression\n",
    "\n",
    "This section is about Regression on cascade-like datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment settings\n",
    "x_size = 12\n",
    "emo_size = 8\n",
    "h_size = 8\n",
    "top_size = (16, 16)\n",
    "\n",
    "# train settings\n",
    "epochs = 60\n",
    "batch_size = 8\n",
    "lr_tree = 0.05\n",
    "lr_top = 0.01\n",
    "decay_tree = 0.003\n",
    "decay_top = 0.006\n",
    "p_drop = 0.1\n",
    "\n",
    "device = th.device('cpu')\n",
    "\n",
    "data_dir = '../data/'\n",
    "graphs_dir = data_dir + 'graphs/'\n",
    "out_dir = '../results/'\n",
    "cascade_size_file = data_dir + 'cascade_size.csv'\n",
    "\n",
    "variant = '1_hour'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.485300\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.585899\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.486926\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.503936\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.518387\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.496302\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.481936\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.522263\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.500883\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.620851\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.489267\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.461187\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.529615\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.578324\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.490229\n",
      "Start experiment with h_size: 3, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.508919\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.463963\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.529544\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.447603\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.485393\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.517508\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.518267\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.477226\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.504810\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.476678\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.490471\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.446939\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.483714\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.494180\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.503459\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.483278\n",
      "Start experiment with h_size: 8, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.545319\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.478556\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.492847\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.476883\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.447669\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.488728\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.506955\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.472597\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.491177\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.464537\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.488425\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.481975\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 8\n",
      "Experiment ended with val_loss: 0.439649\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.515037\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.524366\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.474817\n",
      "Start experiment with h_size: 10, top_size: (16, 16), batch_size: 16\n",
      "Experiment ended with val_loss: 0.486924\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Parameter Tuning\n",
    "# ---------------------------------------------\n",
    "\n",
    "\n",
    "train_ids = np.array([ID.split('_')[0] for ID in os.listdir(graphs_dir) if variant in ID and 'test' not in ID])\n",
    "test_ids = np.unique([ID.split('_')[0] for ID in os.listdir(graphs_dir) if variant + '_test' in ID])\n",
    "\n",
    "train_set = CascadeData(train_ids, data_dir, variant=variant)\n",
    "test_set = CascadeData(test_ids, data_dir, test=True, variant=variant)\n",
    "\n",
    "h_sizes = [3,8,10]\n",
    "batch_sizes = [8,16]\n",
    "dropouts = [0.1, 0.15]\n",
    "lr_trees = [0.01, 0.05]\n",
    "lr_tops = [0.01, 0.05]\n",
    "\n",
    "for h_size, dropout, batch_size, lr_tree, lr_top in product(h_sizes, dropouts, batch_sizes, lr_trees, lr_tops):\n",
    "    print(f'Start experiment with h_size: {h_size}, top_size: {top_size}, batch_size: {batch_size}')\n",
    "    train_generator = DataLoader(train_set, collate_fn=cascade_batcher(device), batch_size= batch_size, num_workers=8)\n",
    "    test_generator = DataLoader(test_set, collate_fn=cascade_batcher(device), batch_size= batch_size, num_workers=8)\n",
    "\n",
    "    deep_tree = DeepTreeLSTMRegressor(x_size, emo_size, h_size=h_size, top_sizes=top_size, pd=dropout)\n",
    "\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer_tree = optim.SGD(deep_tree.bottom_net.parameters(), lr = lr_tree, weight_decay = decay_tree)\n",
    "    optimizer_top = optim.SGD(deep_tree.top_net.parameters(), lr = lr_top, weight_decay = decay_top)\n",
    "    scheduler_tree = optim.lr_scheduler.StepLR(optimizer_tree, step_size=10, gamma=0.8)\n",
    "    scheduler_top = optim.lr_scheduler.StepLR(optimizer_top, step_size=10, gamma=0.8)\n",
    "\n",
    "    callbacks = [EarlyStopping(patience=10),\n",
    "                ExperimentLogger(out_dir, filename='parameter_tuning_4.csv')]\n",
    "\n",
    "    model_trainer = DeepTreeTrainer(deep_tree)\n",
    "    model_trainer.compile(optimizer_tree, optimizer_top, criterion, scheduler_tree=scheduler_tree, scheduler_top=scheduler_top, callbacks=callbacks, metrics=['mae'])\n",
    "    model_trainer.fit(train_generator, test_generator, epochs, verbose=0)\n",
    "    val_loss = min(model_trainer.history['val_loss'])\n",
    "\n",
    "    print(f'Experiment ended with val_loss: {val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start experiment 15_mins run 1/6:\n",
      "Experiment ended with val_loss: 0.594222\n",
      "Start experiment 15_mins run 2/6:\n",
      "Experiment ended with val_loss: 0.589094\n",
      "Start experiment 15_mins run 3/6:\n",
      "Experiment ended with val_loss: 0.595754\n",
      "Start experiment 15_mins run 4/6:\n",
      "Experiment ended with val_loss: 0.618543\n",
      "Start experiment 15_mins run 5/6:\n",
      "Experiment ended with val_loss: 0.599083\n",
      "Start experiment 15_mins run 6/6:\n",
      "Experiment ended with val_loss: 0.627943\n",
      "Start experiment 30_mins run 1/6:\n",
      "Experiment ended with val_loss: 0.600921\n",
      "Start experiment 30_mins run 2/6:\n",
      "Experiment ended with val_loss: 0.544450\n",
      "Start experiment 30_mins run 3/6:\n",
      "Experiment ended with val_loss: 0.504231\n",
      "Start experiment 30_mins run 4/6:\n",
      "Experiment ended with val_loss: 0.592791\n",
      "Start experiment 30_mins run 5/6:\n",
      "Experiment ended with val_loss: 0.546265\n",
      "Start experiment 30_mins run 6/6:\n",
      "Experiment ended with val_loss: 0.525734\n",
      "Start experiment 1_hour run 1/6:\n",
      "Experiment ended with val_loss: 0.487719\n",
      "Start experiment 1_hour run 2/6:\n",
      "Experiment ended with val_loss: 0.454451\n",
      "Start experiment 1_hour run 3/6:\n",
      "Experiment ended with val_loss: 0.413863\n",
      "Start experiment 1_hour run 4/6:\n",
      "Experiment ended with val_loss: 0.464551\n",
      "Start experiment 1_hour run 5/6:\n",
      "Experiment ended with val_loss: 0.407777\n",
      "Start experiment 1_hour run 6/6:\n",
      "Experiment ended with val_loss: 0.465743\n",
      "Start experiment 3_hour run 1/6:\n",
      "Experiment ended with val_loss: 0.374188\n",
      "Start experiment 3_hour run 2/6:\n",
      "Experiment ended with val_loss: 0.293762\n",
      "Start experiment 3_hour run 3/6:\n",
      "Experiment ended with val_loss: 0.322828\n",
      "Start experiment 3_hour run 4/6:\n",
      "Experiment ended with val_loss: 0.311187\n",
      "Start experiment 3_hour run 5/6:\n",
      "Experiment ended with val_loss: 0.298899\n",
      "Start experiment 3_hour run 6/6:\n",
      "Experiment ended with val_loss: 0.304487\n",
      "Start experiment 24_hour run 1/6:\n",
      "Experiment ended with val_loss: 0.229099\n",
      "Start experiment 24_hour run 2/6:\n",
      "Experiment ended with val_loss: 0.213099\n",
      "Start experiment 24_hour run 3/6:\n",
      "Experiment ended with val_loss: 0.229476\n",
      "Start experiment 24_hour run 4/6:\n",
      "Experiment ended with val_loss: 0.212145\n",
      "Start experiment 24_hour run 5/6:\n",
      "Experiment ended with val_loss: 0.214656\n",
      "Start experiment 24_hour run 6/6:\n",
      "Experiment ended with val_loss: 0.216096\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Experiment 1\n",
    "# ---------------------------------------------\n",
    "\n",
    "variants = ['15_mins', '30_mins', '1_hour', '3_hour', '24_hour']\n",
    "\n",
    "for variant in variants:\n",
    "    train_ids = np.array([ID.split('_')[0] for ID in os.listdir(graphs_dir) if variant in ID and 'test' not in ID])\n",
    "    test_ids = np.unique([ID.split('_')[0] for ID in os.listdir(graphs_dir) if variant + '_test' in ID])\n",
    "\n",
    "    train_set = CascadeData(train_ids, graphs_dir, cascade_size_file, variant=variant)\n",
    "    test_set = CascadeData(test_ids, graphs_dir, cascade_size_file, test=True, variant=variant)\n",
    "\n",
    "\n",
    "    for i in range(6):\n",
    "        print(f'Start experiment {variant} run {i+1}/6:')\n",
    "        train_generator = DataLoader(train_set, collate_fn=cascade_batcher(device), batch_size= batch_size, num_workers=8)\n",
    "        test_generator = DataLoader(test_set, collate_fn=cascade_batcher(device), batch_size= batch_size, num_workers=8)\n",
    "\n",
    "        deep_tree = DeepTreeLSTMRegressor(x_size, emo_size, h_size=h_size, top_sizes=top_size, pd=p_drop)\n",
    "\n",
    "        criterion = nn.MSELoss().to(device)\n",
    "        optimizer_tree = optim.SGD(deep_tree.bottom_net.parameters(), lr = lr_tree, weight_decay = decay_tree)\n",
    "        optimizer_top = optim.SGD(deep_tree.top_net.parameters(), lr = lr_top, weight_decay = decay_top)\n",
    "        scheduler_tree = optim.lr_scheduler.StepLR(optimizer_tree, step_size=10, gamma=0.8)\n",
    "        scheduler_top = optim.lr_scheduler.StepLR(optimizer_top, step_size=10, gamma=0.8)\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=10),\n",
    "                    ExperimentLogger(out_dir, filename='experiment_2.csv')]\n",
    "\n",
    "        model_trainer = DeepTreeTrainer(deep_tree)\n",
    "        model_trainer.compile(optimizer_tree, optimizer_top, criterion, scheduler_tree=scheduler_tree, scheduler_top=scheduler_top, callbacks=callbacks, metrics=['mae'])\n",
    "        model_trainer.fit(train_generator, test_generator, epochs, verbose=0)\n",
    "        val_loss = min(model_trainer.history['val_loss'])\n",
    "\n",
    "        print(f'Experiment ended with val_loss: {val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start experiment structureless run 1/6:\n",
      "Experiment ended with val_loss: 0.447984\n",
      "Start experiment structureless run 2/6:\n",
      "Experiment ended with val_loss: 0.443082\n",
      "Start experiment structureless run 3/6:\n",
      "Experiment ended with val_loss: 0.488214\n",
      "Start experiment structureless run 4/6:\n",
      "Experiment ended with val_loss: 0.478036\n",
      "Start experiment structureless run 5/6:\n",
      "Experiment ended with val_loss: 0.475786\n",
      "Start experiment structureless run 6/6:\n",
      "Experiment ended with val_loss: 0.439464\n",
      "Start experiment regular run 1/6:\n",
      "Experiment ended with val_loss: 0.439656\n",
      "Start experiment regular run 2/6:\n",
      "Experiment ended with val_loss: 0.441569\n",
      "Start experiment regular run 3/6:\n",
      "Experiment ended with val_loss: 0.461005\n",
      "Start experiment regular run 4/6:\n",
      "Experiment ended with val_loss: 0.425366\n",
      "Start experiment regular run 5/6:\n",
      "Experiment ended with val_loss: 0.455729\n",
      "Start experiment regular run 6/6:\n",
      "Experiment ended with val_loss: 0.445083\n",
      "Start experiment unidirectional run 1/6:\n",
      "Experiment ended with val_loss: 0.526426\n",
      "Start experiment unidirectional run 2/6:\n",
      "Experiment ended with val_loss: 0.509122\n",
      "Start experiment unidirectional run 3/6:\n",
      "Experiment ended with val_loss: 0.502629\n",
      "Start experiment unidirectional run 4/6:\n",
      "Experiment ended with val_loss: 0.524290\n",
      "Start experiment unidirectional run 5/6:\n",
      "Experiment ended with val_loss: 0.535604\n",
      "Start experiment unidirectional run 6/6:\n",
      "Experiment ended with val_loss: 0.485052\n",
      "Start experiment no_emotions run 1/6:\n",
      "Experiment ended with val_loss: 0.419580\n",
      "Start experiment no_emotions run 2/6:\n",
      "Experiment ended with val_loss: 0.431843\n",
      "Start experiment no_emotions run 3/6:\n",
      "Experiment ended with val_loss: 0.451666\n",
      "Start experiment no_emotions run 4/6:\n",
      "Experiment ended with val_loss: 0.418188\n",
      "Start experiment no_emotions run 5/6:\n",
      "Experiment ended with val_loss: 0.384298\n",
      "Start experiment no_emotions run 6/6:\n",
      "Experiment ended with val_loss: 0.414830\n",
      "Start experiment regular_tree run 1/6:\n",
      "Experiment ended with val_loss: 0.595729\n",
      "Start experiment regular_tree run 2/6:\n",
      "Experiment ended with val_loss: 0.582741\n",
      "Start experiment regular_tree run 3/6:\n",
      "Experiment ended with val_loss: 0.574039\n",
      "Start experiment regular_tree run 4/6:\n",
      "Experiment ended with val_loss: 0.556855\n",
      "Start experiment regular_tree run 5/6:\n",
      "Experiment ended with val_loss: 0.533171\n",
      "Start experiment regular_tree run 6/6:\n",
      "Experiment ended with val_loss: 0.579787\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Experiment 1: Model Variations\n",
    "# ---------------------------------------------\n",
    "\n",
    "variant = '1_hour'\n",
    "variation_graphs_dir = data_dir + 'graphs_variations/'\n",
    "\n",
    "\n",
    "variations = { #(structureless, bi, deep)\n",
    "    'structureless' : (True, True, True),\n",
    "    'regular' : (False, True, True),\n",
    "    'unidirectional' : (False, False, True),\n",
    "    'no_emotions' : (False, True, False),\n",
    "    'regular_tree' : (False, False, False)\n",
    "}\n",
    "\n",
    "for variation, (structureless, bi, deep) in variations.items():\n",
    "    train_ids = np.unique([ID.split('_')[0] for ID in os.listdir(variation_graphs_dir) if variant in ID and 'test' not in ID])\n",
    "    test_ids = np.unique([ID.split('_')[0] for ID in os.listdir(variation_graphs_dir) if variant + '_test' in ID])\n",
    "\n",
    "    train_set = CascadeData(train_ids, variation_graphs_dir, cascade_size_file, variant=variant, structureless=structureless)\n",
    "    test_set = CascadeData(test_ids, variation_graphs_dir, cascade_size_file, test=True, variant=variant, structureless=structureless)\n",
    "\n",
    "    x_size_variations = x_size\n",
    "    if structureless:\n",
    "        x_size_variations = x_size - 2\n",
    "\n",
    "    for i in range(6):\n",
    "        print(f'Start experiment {variation} run {i+1}/6:')\n",
    "        train_generator = DataLoader(train_set, collate_fn=cascade_batcher(device), batch_size= batch_size, num_workers=8)\n",
    "        test_generator = DataLoader(test_set, collate_fn=cascade_batcher(device), batch_size= batch_size, num_workers=8)\n",
    "\n",
    "\n",
    "        deep_tree = DeepTreeLSTMRegressor(x_size_variations, emo_size, h_size=h_size, top_sizes=top_size, pd=p_drop, bi=bi, deep=deep)\n",
    "\n",
    "        criterion = nn.MSELoss().to(device)\n",
    "        optimizer_tree = optim.SGD(deep_tree.bottom_net.parameters(), lr = lr_tree, weight_decay = decay_tree)\n",
    "        optimizer_top = optim.SGD(deep_tree.top_net.parameters(), lr = lr_top, weight_decay = decay_top)\n",
    "        scheduler_tree = optim.lr_scheduler.StepLR(optimizer_tree, step_size=10, gamma=0.8)\n",
    "        scheduler_top = optim.lr_scheduler.StepLR(optimizer_top, step_size=10, gamma=0.8)\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=10),\n",
    "                    ExperimentLogger(out_dir, filename='experiment_2_variations.csv')]\n",
    "\n",
    "        model_trainer = DeepTreeTrainer(deep_tree)\n",
    "        model_trainer.compile(optimizer_tree, optimizer_top, criterion, scheduler_tree=scheduler_tree, scheduler_top=scheduler_top, callbacks=callbacks, metrics=['mae'])\n",
    "        model_trainer.fit(train_generator, test_generator, epochs, verbose=0)\n",
    "        val_loss = min(model_trainer.history['val_loss'])\n",
    "\n",
    "        print(f'Experiment ended with val_loss: {val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/60\n",
      "234/234 [########################################] - 23s 99ms/step - loss: 3.2964 - val_loss: 2.4612 - val_mae: 1.0326 \n",
      "Epoch: 2/60\n",
      "234/234 [########################################] - 27s 117ms/step - loss: 1.5789 - val_loss: 1.2101 - val_mae: 0.7673 \n",
      "Epoch: 3/60\n",
      "234/234 [########################################] - 27s 118ms/step - loss: 1.2186 - val_loss: 1.3457 - val_mae: 0.7886 \n",
      "Epoch: 4/60\n",
      "234/234 [########################################] - 28s 121ms/step - loss: 1.0744 - val_loss: 1.1446 - val_mae: 0.7396 \n",
      "Epoch: 5/60\n",
      "234/234 [########################################] - 27s 119ms/step - loss: 1.0203 - val_loss: 0.6568 - val_mae: 0.5837 \n",
      "Epoch: 6/60\n",
      "234/234 [########################################] - 27s 119ms/step - loss: 0.8297 - val_loss: 0.7062 - val_mae: 0.5914 \n",
      "Epoch: 7/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.7789 - val_loss: 0.6477 - val_mae: 0.5769 \n",
      "Epoch: 8/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.7793 - val_loss: 0.7721 - val_mae: 0.6227 \n",
      "Epoch: 9/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.7375 - val_loss: 0.8045 - val_mae: 0.6257 \n",
      "Epoch: 10/60\n",
      "234/234 [########################################] - 28s 121ms/step - loss: 0.7249 - val_loss: 1.0041 - val_mae: 0.7070 \n",
      "Epoch: 11/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.6784 - val_loss: 0.6746 - val_mae: 0.5801 \n",
      "Epoch: 12/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.6875 - val_loss: 0.6472 - val_mae: 0.5692 \n",
      "Epoch: 13/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.6827 - val_loss: 0.5317 - val_mae: 0.5296 \n",
      "Epoch: 14/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.6244 - val_loss: 0.5498 - val_mae: 0.5381 \n",
      "Epoch: 15/60\n",
      "234/234 [########################################] - 28s 121ms/step - loss: 0.6549 - val_loss: 0.6073 - val_mae: 0.5528 \n",
      "Epoch: 16/60\n",
      "234/234 [########################################] - 28s 121ms/step - loss: 0.6198 - val_loss: 0.5718 - val_mae: 0.5433 \n",
      "Epoch: 17/60\n",
      "234/234 [########################################] - 28s 122ms/step - loss: 0.6130 - val_loss: 0.6042 - val_mae: 0.5519 \n",
      "Epoch: 18/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.6319 - val_loss: 0.5366 - val_mae: 0.5326 \n",
      "Epoch: 19/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.6225 - val_loss: 0.5459 - val_mae: 0.5449 \n",
      "Epoch: 20/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.6007 - val_loss: 0.5734 - val_mae: 0.5418 \n",
      "Epoch: 21/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.5641 - val_loss: 0.4839 - val_mae: 0.5075 \n",
      "Epoch: 22/60\n",
      "234/234 [########################################] - 28s 121ms/step - loss: 0.5732 - val_loss: 0.5069 - val_mae: 0.5146 \n",
      "Epoch: 23/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.5490 - val_loss: 0.6200 - val_mae: 0.5736 \n",
      "Epoch: 24/60\n",
      "234/234 [########################################] - 28s 120ms/step - loss: 0.5569 - val_loss: 0.5383 - val_mae: 0.5423 \n",
      "Epoch: 25/60\n",
      "234/234 [########################################] - 28s 123ms/step - loss: 0.5488 - val_loss: 0.5508 - val_mae: 0.5325 \n",
      "Epoch: 26/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.5500 - val_loss: 0.6218 - val_mae: 0.5742 \n",
      "Epoch: 27/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.5620 - val_loss: 0.5955 - val_mae: 0.5507 \n",
      "Epoch: 28/60\n",
      "234/234 [########################################] - 26s 115ms/step - loss: 0.5526 - val_loss: 0.6211 - val_mae: 0.5689 \n",
      "Epoch: 29/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.5552 - val_loss: 0.5700 - val_mae: 0.5513 \n",
      "Epoch: 30/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.5410 - val_loss: 0.4670 - val_mae: 0.5133 \n",
      "Epoch: 31/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.5402 - val_loss: 0.5240 - val_mae: 0.5277 \n",
      "Epoch: 32/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.5504 - val_loss: 0.4947 - val_mae: 0.5194 \n",
      "Epoch: 33/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.5268 - val_loss: 0.5466 - val_mae: 0.5414 \n",
      "Epoch: 34/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.5427 - val_loss: 0.5115 - val_mae: 0.5217 \n",
      "Epoch: 35/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.5381 - val_loss: 0.5456 - val_mae: 0.5453 \n",
      "Epoch: 36/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.5277 - val_loss: 0.4677 - val_mae: 0.5094 \n",
      "Epoch: 37/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.5079 - val_loss: 0.5467 - val_mae: 0.5366 \n",
      "Epoch: 38/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.5429 - val_loss: 0.4344 - val_mae: 0.4999 \n",
      "Epoch: 39/60\n",
      "234/234 [########################################] - 26s 115ms/step - loss: 0.5017 - val_loss: 0.4710 - val_mae: 0.5072 \n",
      "Epoch: 40/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.5248 - val_loss: 0.5606 - val_mae: 0.5445 \n",
      "Epoch: 41/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.4934 - val_loss: 0.4571 - val_mae: 0.5012 \n",
      "Epoch: 42/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.4687 - val_loss: 0.4502 - val_mae: 0.5009 \n",
      "Epoch: 43/60\n",
      "234/234 [########################################] - 26s 115ms/step - loss: 0.4832 - val_loss: 0.4694 - val_mae: 0.5036 \n",
      "Epoch: 44/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.4985 - val_loss: 0.5356 - val_mae: 0.5402 \n",
      "Epoch: 45/60\n",
      "234/234 [########################################] - 28s 123ms/step - loss: 0.4722 - val_loss: 0.4465 - val_mae: 0.4973 \n",
      "Epoch: 46/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.4896 - val_loss: 0.4730 - val_mae: 0.5072 \n",
      "Epoch: 47/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.4930 - val_loss: 0.4495 - val_mae: 0.4953 \n",
      "Epoch: 48/60\n",
      "234/234 [########################################] - 26s 115ms/step - loss: 0.4876 - val_loss: 0.4443 - val_mae: 0.4964 \n",
      "Epoch: 49/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.4987 - val_loss: 0.4618 - val_mae: 0.4995 \n",
      "Epoch: 50/60\n",
      "234/234 [########################################] - 27s 116ms/step - loss: 0.4919 - val_loss: 0.4479 - val_mae: 0.4957 \n",
      "Epoch: 51/60\n",
      "234/234 [########################################] - 27s 118ms/step - loss: 0.4616 - val_loss: 0.4196 - val_mae: 0.4894 \n",
      "Epoch: 52/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.4805 - val_loss: 0.4277 - val_mae: 0.4853 \n",
      "Epoch: 53/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.4596 - val_loss: 0.4708 - val_mae: 0.5033 \n",
      "Epoch: 54/60\n",
      "234/234 [########################################] - 26s 115ms/step - loss: 0.4655 - val_loss: 0.4266 - val_mae: 0.4907 \n",
      "Epoch: 55/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.5078 - val_loss: 0.4736 - val_mae: 0.5200 \n",
      "Epoch: 56/60\n",
      "234/234 [########################################] - 26s 115ms/step - loss: 0.4494 - val_loss: 0.4613 - val_mae: 0.5113 \n",
      "Epoch: 57/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.4655 - val_loss: 0.4566 - val_mae: 0.5040 \n",
      "Epoch: 58/60\n",
      "234/234 [########################################] - 26s 115ms/step - loss: 0.4765 - val_loss: 0.4716 - val_mae: 0.5085 \n",
      "Epoch: 59/60\n",
      "234/234 [########################################] - 26s 114ms/step - loss: 0.4689 - val_loss: 0.4877 - val_mae: 0.5096 \n",
      "Epoch: 60/60\n",
      "234/234 [########################################] - 26s 113ms/step - loss: 0.4815 - val_loss: 0.4542 - val_mae: 0.4963 \n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Single run\n",
    "# ---------------------------------------------\n",
    "\n",
    "train_ids = np.array([ID.split('_')[0] for ID in os.listdir(graphs_dir) if variant in ID and 'test' not in ID])\n",
    "test_ids = np.unique([ID.split('_')[0] for ID in os.listdir(graphs_dir) if variant + '_test' in ID])\n",
    "\n",
    "train_set = CascadeData(train_ids, graphs_dir, cascade_size_file, variant=variant)\n",
    "test_set = CascadeData(test_ids, graphs_dir, cascade_size_file, test=True, variant=variant)\n",
    "\n",
    "train_generator = DataLoader(train_set, collate_fn=cascade_batcher(device), batch_size= batch_size, num_workers=8)\n",
    "test_generator = DataLoader(test_set, collate_fn=cascade_batcher(device), batch_size= batch_size, num_workers=8)\n",
    "\n",
    "deep_tree = DeepTreeLSTMRegressor(x_size, emo_size, h_size=h_size, top_sizes=top_size, pd=p_drop)\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer_tree = optim.SGD(deep_tree.bottom_net.parameters(), lr = lr_tree, weight_decay = decay_tree)\n",
    "optimizer_top = optim.SGD(deep_tree.top_net.parameters(), lr = lr_top, weight_decay = decay_top)\n",
    "scheduler_tree = optim.lr_scheduler.StepLR(optimizer_tree, step_size=10, gamma=0.8)\n",
    "scheduler_top = optim.lr_scheduler.StepLR(optimizer_top, step_size=10, gamma=0.8)\n",
    "\n",
    "callbacks = [ExperimentLogger(out_dir)]\n",
    "\n",
    "model_trainer = DeepTreeTrainer(deep_tree)\n",
    "model_trainer.compile(optimizer_tree, optimizer_top, criterion, scheduler_tree=scheduler_tree, scheduler_top=scheduler_top, callbacks=callbacks, metrics=['mae'])\n",
    "model_trainer.fit(train_generator, test_generator, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"260.631875pt\" version=\"1.1\" viewBox=\"0 0 384.839375 260.631875\" width=\"384.839375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-12-31T16:11:35.332068</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 260.631875 \nL 384.839375 260.631875 \nL 384.839375 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 42.839375 224.64 \nL 377.639375 224.64 \nL 377.639375 7.2 \nL 42.839375 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 52.809908 224.64 \nL 52.809908 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\"/>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(49.310533 236.498281)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 31.78125 3.421875 \nQ 39.265625 3.421875 42.96875 11.625 \nQ 46.6875 19.828125 46.6875 36.375 \nQ 46.6875 52.984375 42.96875 61.1875 \nQ 39.265625 69.390625 31.78125 69.390625 \nQ 24.3125 69.390625 20.59375 61.1875 \nQ 16.890625 52.984375 16.890625 36.375 \nQ 16.890625 19.828125 20.59375 11.625 \nQ 24.3125 3.421875 31.78125 3.421875 \nz\nM 31.78125 -1.421875 \nQ 19.921875 -1.421875 13.25 8.53125 \nQ 6.59375 18.5 6.59375 36.375 \nQ 6.59375 54.296875 13.25 64.25 \nQ 19.921875 74.21875 31.78125 74.21875 \nQ 43.703125 74.21875 50.34375 64.25 \nQ 56.984375 54.296875 56.984375 36.375 \nQ 56.984375 18.5 50.34375 8.53125 \nQ 43.703125 -1.421875 31.78125 -1.421875 \nz\n\" id=\"DejaVuSerif-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 105.286397 224.64 \nL 105.286397 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\"/>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g style=\"fill:#262626;\" transform=\"translate(98.287647 236.498281)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 14.203125 0 \nL 14.203125 5.171875 \nL 26.90625 5.171875 \nL 26.90625 65.828125 \nL 12.203125 56.296875 \nL 12.203125 62.703125 \nL 29.984375 74.21875 \nL 36.71875 74.21875 \nL 36.71875 5.171875 \nL 49.421875 5.171875 \nL 49.421875 0 \nz\n\" id=\"DejaVuSerif-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 157.762886 224.64 \nL 157.762886 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\"/>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g style=\"fill:#262626;\" transform=\"translate(150.764136 236.498281)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 12.796875 55.515625 \nL 7.328125 55.515625 \nL 7.328125 68.5 \nQ 12.546875 71.296875 17.84375 72.75 \nQ 23.140625 74.21875 28.21875 74.21875 \nQ 39.59375 74.21875 46.1875 68.703125 \nQ 52.78125 63.1875 52.78125 53.71875 \nQ 52.78125 43.015625 37.84375 28.125 \nQ 36.671875 27 36.078125 26.421875 \nL 17.671875 8.015625 \nL 48.09375 8.015625 \nL 48.09375 17 \nL 53.8125 17 \nL 53.8125 0 \nL 6.78125 0 \nL 6.78125 5.328125 \nL 28.90625 27.390625 \nQ 36.234375 34.71875 39.359375 40.84375 \nQ 42.484375 46.96875 42.484375 53.71875 \nQ 42.484375 61.078125 38.640625 65.234375 \nQ 34.8125 69.390625 28.078125 69.390625 \nQ 21.09375 69.390625 17.28125 65.921875 \nQ 13.484375 62.453125 12.796875 55.515625 \nz\n\" id=\"DejaVuSerif-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 210.239375 224.64 \nL 210.239375 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\"/>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g style=\"fill:#262626;\" transform=\"translate(203.240625 236.498281)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 9.71875 69.828125 \nQ 15.4375 71.96875 20.671875 73.09375 \nQ 25.921875 74.21875 30.515625 74.21875 \nQ 41.21875 74.21875 47.21875 69.59375 \nQ 53.21875 64.984375 53.21875 56.78125 \nQ 53.21875 50.203125 49.0625 45.78125 \nQ 44.921875 41.359375 37.3125 39.796875 \nQ 46.296875 38.53125 51.25 33.28125 \nQ 56.203125 28.03125 56.203125 19.671875 \nQ 56.203125 9.46875 49.34375 4.015625 \nQ 42.484375 -1.421875 29.59375 -1.421875 \nQ 23.875 -1.421875 18.421875 -0.1875 \nQ 12.984375 1.03125 7.625 3.515625 \nL 7.625 17.671875 \nL 13.09375 17.671875 \nQ 13.578125 10.640625 17.828125 7.03125 \nQ 22.078125 3.421875 29.78125 3.421875 \nQ 37.25 3.421875 41.578125 7.734375 \nQ 45.90625 12.0625 45.90625 19.578125 \nQ 45.90625 28.171875 41.453125 32.59375 \nQ 37.015625 37.015625 28.421875 37.015625 \nL 23.78125 37.015625 \nL 23.78125 42 \nL 26.21875 42 \nQ 34.765625 42 39.03125 45.53125 \nQ 43.3125 49.078125 43.3125 56.203125 \nQ 43.3125 62.59375 39.796875 65.984375 \nQ 36.28125 69.390625 29.6875 69.390625 \nQ 23.09375 69.390625 19.453125 66.265625 \nQ 15.828125 63.140625 15.1875 56.984375 \nL 9.71875 56.984375 \nz\n\" id=\"DejaVuSerif-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 262.715864 224.64 \nL 262.715864 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\"/>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(255.717114 236.498281)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 34.90625 24.703125 \nL 34.90625 63.484375 \nL 10.015625 24.703125 \nz\nM 56.390625 0 \nL 23.1875 0 \nL 23.1875 5.171875 \nL 34.90625 5.171875 \nL 34.90625 19.484375 \nL 3.078125 19.484375 \nL 3.078125 24.8125 \nL 35.015625 74.21875 \nL 44.671875 74.21875 \nL 44.671875 24.703125 \nL 58.59375 24.703125 \nL 58.59375 19.484375 \nL 44.671875 19.484375 \nL 44.671875 5.171875 \nL 56.390625 5.171875 \nz\n\" id=\"DejaVuSerif-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 315.192353 224.64 \nL 315.192353 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\"/>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g style=\"fill:#262626;\" transform=\"translate(308.193603 236.498281)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 50.296875 72.90625 \nL 50.296875 64.890625 \nL 16.890625 64.890625 \nL 16.890625 44 \nQ 19.4375 45.75 22.828125 46.625 \nQ 26.21875 47.515625 30.421875 47.515625 \nQ 42.234375 47.515625 49.0625 40.96875 \nQ 55.90625 34.421875 55.90625 23.09375 \nQ 55.90625 11.53125 49 5.046875 \nQ 42.09375 -1.421875 29.59375 -1.421875 \nQ 24.5625 -1.421875 19.28125 -0.1875 \nQ 14.015625 1.03125 8.5 3.515625 \nL 8.5 17.671875 \nL 14.015625 17.671875 \nQ 14.453125 10.75 18.421875 7.078125 \nQ 22.40625 3.421875 29.59375 3.421875 \nQ 37.3125 3.421875 41.453125 8.5 \nQ 45.609375 13.578125 45.609375 23.09375 \nQ 45.609375 32.5625 41.484375 37.609375 \nQ 37.359375 42.671875 29.59375 42.671875 \nQ 25.203125 42.671875 21.84375 41.109375 \nQ 18.5 39.546875 15.921875 36.28125 \nL 11.71875 36.28125 \nL 11.71875 72.90625 \nz\n\" id=\"DejaVuSerif-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 367.668842 224.64 \nL 367.668842 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\"/>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <g style=\"fill:#262626;\" transform=\"translate(360.670092 236.498281)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 32.71875 3.421875 \nQ 39.59375 3.421875 43.296875 8.46875 \nQ 47.015625 13.53125 47.015625 23 \nQ 47.015625 32.46875 43.296875 37.515625 \nQ 39.59375 42.578125 32.71875 42.578125 \nQ 25.734375 42.578125 22.0625 37.6875 \nQ 18.40625 32.8125 18.40625 23.578125 \nQ 18.40625 13.875 22.109375 8.640625 \nQ 25.828125 3.421875 32.71875 3.421875 \nz\nM 16.796875 40.140625 \nQ 20.125 43.796875 24.3125 45.59375 \nQ 28.515625 47.40625 33.796875 47.40625 \nQ 44.671875 47.40625 51 40.859375 \nQ 57.328125 34.328125 57.328125 23 \nQ 57.328125 11.921875 50.515625 5.25 \nQ 43.703125 -1.421875 32.328125 -1.421875 \nQ 19.96875 -1.421875 13.328125 7.78125 \nQ 6.6875 17 6.6875 34.078125 \nQ 6.6875 53.21875 14.546875 63.71875 \nQ 22.40625 74.21875 36.71875 74.21875 \nQ 40.578125 74.21875 44.828125 73.484375 \nQ 49.078125 72.75 53.515625 71.296875 \nL 53.515625 59.28125 \nL 48 59.28125 \nQ 47.40625 64.203125 44.234375 66.796875 \nQ 41.0625 69.390625 35.6875 69.390625 \nQ 26.21875 69.390625 21.578125 62.203125 \nQ 16.9375 55.03125 16.796875 40.140625 \nz\n\" id=\"DejaVuSerif-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Epoch -->\n     <g style=\"fill:#262626;\" transform=\"translate(192.77 251.144219)scale(0.11 -0.11)\">\n      <defs>\n       <path d=\"M 5.515625 0 \nL 5.515625 5.171875 \nL 14.796875 5.171875 \nL 14.796875 67.671875 \nL 5.515625 67.671875 \nL 5.515625 72.90625 \nL 64.203125 72.90625 \nL 64.203125 56.6875 \nL 58.203125 56.6875 \nL 58.203125 66.890625 \nL 24.703125 66.890625 \nL 24.703125 42.484375 \nL 48.578125 42.484375 \nL 48.578125 51.609375 \nL 54.59375 51.609375 \nL 54.59375 27.390625 \nL 48.578125 27.390625 \nL 48.578125 36.53125 \nL 24.703125 36.53125 \nL 24.703125 6 \nL 58.984375 6 \nL 58.984375 16.21875 \nL 64.984375 16.21875 \nL 64.984375 0 \nz\n\" id=\"DejaVuSerif-69\"/>\n       <path d=\"M 20.515625 28.515625 \nL 20.515625 23.390625 \nQ 20.515625 14.015625 24.09375 9.109375 \nQ 27.6875 4.203125 34.515625 4.203125 \nQ 41.40625 4.203125 44.9375 9.71875 \nQ 48.484375 15.234375 48.484375 25.984375 \nQ 48.484375 36.765625 44.9375 42.234375 \nQ 41.40625 47.703125 34.515625 47.703125 \nQ 27.6875 47.703125 24.09375 42.765625 \nQ 20.515625 37.84375 20.515625 28.515625 \nz\nM 11.53125 46.6875 \nL 2.875 46.6875 \nL 2.875 51.90625 \nL 20.515625 51.90625 \nL 20.515625 43.796875 \nQ 23.140625 48.6875 27.21875 51 \nQ 31.296875 53.328125 37.3125 53.328125 \nQ 46.875 53.328125 52.921875 45.75 \nQ 58.984375 38.1875 58.984375 25.984375 \nQ 58.984375 13.765625 52.921875 6.171875 \nQ 46.875 -1.421875 37.3125 -1.421875 \nQ 31.296875 -1.421875 27.21875 0.890625 \nQ 23.140625 3.21875 20.515625 8.109375 \nL 20.515625 -15.578125 \nL 29 -15.578125 \nL 29 -20.796875 \nL 2.875 -20.796875 \nL 2.875 -15.578125 \nL 11.53125 -15.578125 \nz\n\" id=\"DejaVuSerif-112\"/>\n       <path d=\"M 30.078125 3.421875 \nQ 37.3125 3.421875 40.984375 9.125 \nQ 44.671875 14.84375 44.671875 25.984375 \nQ 44.671875 37.109375 40.984375 42.796875 \nQ 37.3125 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 19.171875 42.796875 \nQ 15.484375 37.109375 15.484375 25.984375 \nQ 15.484375 14.84375 19.1875 9.125 \nQ 22.90625 3.421875 30.078125 3.421875 \nz\nM 30.078125 -1.421875 \nQ 18.75 -1.421875 11.859375 6.078125 \nQ 4.984375 13.578125 4.984375 25.984375 \nQ 4.984375 38.375 11.84375 45.84375 \nQ 18.703125 53.328125 30.078125 53.328125 \nQ 41.453125 53.328125 48.3125 45.84375 \nQ 55.171875 38.375 55.171875 25.984375 \nQ 55.171875 13.578125 48.3125 6.078125 \nQ 41.453125 -1.421875 30.078125 -1.421875 \nz\n\" id=\"DejaVuSerif-111\"/>\n       <path d=\"M 51.421875 15.578125 \nQ 49.515625 7.28125 44.09375 2.921875 \nQ 38.671875 -1.421875 30.078125 -1.421875 \nQ 18.75 -1.421875 11.859375 6.078125 \nQ 4.984375 13.578125 4.984375 25.984375 \nQ 4.984375 38.421875 11.859375 45.875 \nQ 18.75 53.328125 30.078125 53.328125 \nQ 35.015625 53.328125 39.890625 52.171875 \nQ 44.78125 51.03125 49.703125 48.6875 \nL 49.703125 35.40625 \nL 44.484375 35.40625 \nQ 43.453125 42.234375 40.015625 45.359375 \nQ 36.578125 48.484375 30.171875 48.484375 \nQ 22.90625 48.484375 19.1875 42.84375 \nQ 15.484375 37.203125 15.484375 25.984375 \nQ 15.484375 14.75 19.171875 9.078125 \nQ 22.859375 3.421875 30.171875 3.421875 \nQ 35.984375 3.421875 39.453125 6.4375 \nQ 42.921875 9.46875 44.1875 15.578125 \nz\n\" id=\"DejaVuSerif-99\"/>\n       <path d=\"M 4.109375 0 \nL 4.109375 5.171875 \nL 12.203125 5.171875 \nL 12.203125 70.796875 \nL 3.609375 70.796875 \nL 3.609375 75.984375 \nL 21.1875 75.984375 \nL 21.1875 42.671875 \nQ 23.6875 47.953125 27.65625 50.640625 \nQ 31.640625 53.328125 36.921875 53.328125 \nQ 45.515625 53.328125 49.5625 48.390625 \nQ 53.609375 43.453125 53.609375 33.015625 \nL 53.609375 5.171875 \nL 61.625 5.171875 \nL 61.625 0 \nL 36.8125 0 \nL 36.8125 5.171875 \nL 44.578125 5.171875 \nL 44.578125 30.171875 \nQ 44.578125 39.703125 42.25 43.1875 \nQ 39.9375 46.6875 33.984375 46.6875 \nQ 27.734375 46.6875 24.453125 42.140625 \nQ 21.1875 37.59375 21.1875 28.90625 \nL 21.1875 5.171875 \nL 29 5.171875 \nL 29 0 \nz\n\" id=\"DejaVuSerif-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-69\"/>\n      <use x=\"72.998047\" xlink:href=\"#DejaVuSerif-112\"/>\n      <use x=\"137.011719\" xlink:href=\"#DejaVuSerif-111\"/>\n      <use x=\"197.216797\" xlink:href=\"#DejaVuSerif-99\"/>\n      <use x=\"253.222656\" xlink:href=\"#DejaVuSerif-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 42.839375 218.102973 \nL 377.639375 218.102973 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\"/>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(21.845938 222.282113)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 9.421875 5.078125 \nQ 9.421875 7.8125 11.28125 9.71875 \nQ 13.140625 11.625 15.921875 11.625 \nQ 18.609375 11.625 20.5 9.71875 \nQ 22.40625 7.8125 22.40625 5.078125 \nQ 22.40625 2.390625 20.5 0.484375 \nQ 18.609375 -1.421875 15.921875 -1.421875 \nQ 13.140625 -1.421875 11.28125 0.453125 \nQ 9.421875 2.34375 9.421875 5.078125 \nz\n\" id=\"DejaVuSerif-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 42.839375 184.001221 \nL 377.639375 184.001221 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\"/>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g style=\"fill:#262626;\" transform=\"translate(21.845938 188.180362)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 42.839375 149.89947 \nL 377.639375 149.89947 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\"/>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g style=\"fill:#262626;\" transform=\"translate(21.845938 154.078611)scale(0.11 -0.11)\">\n       <defs>\n        <path d=\"M 46.578125 19.921875 \nQ 46.578125 27.734375 42.6875 32.046875 \nQ 38.8125 36.375 31.78125 36.375 \nQ 24.75 36.375 20.875 32.046875 \nQ 17 27.734375 17 19.921875 \nQ 17 12.0625 20.875 7.734375 \nQ 24.75 3.421875 31.78125 3.421875 \nQ 38.8125 3.421875 42.6875 7.734375 \nQ 46.578125 12.0625 46.578125 19.921875 \nz\nM 44.578125 55.328125 \nQ 44.578125 61.96875 41.203125 65.671875 \nQ 37.84375 69.390625 31.78125 69.390625 \nQ 25.78125 69.390625 22.390625 65.671875 \nQ 19 61.96875 19 55.328125 \nQ 19 48.640625 22.390625 44.921875 \nQ 25.78125 41.21875 31.78125 41.21875 \nQ 37.84375 41.21875 41.203125 44.921875 \nQ 44.578125 48.640625 44.578125 55.328125 \nz\nM 39.3125 38.8125 \nQ 47.609375 37.703125 52.25 32.6875 \nQ 56.890625 27.6875 56.890625 19.921875 \nQ 56.890625 9.671875 50.390625 4.125 \nQ 43.890625 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.203125 4.125 \nQ 6.6875 9.671875 6.6875 19.921875 \nQ 6.6875 27.6875 11.328125 32.6875 \nQ 15.96875 37.703125 24.3125 38.8125 \nQ 16.9375 40.140625 13 44.40625 \nQ 9.078125 48.6875 9.078125 55.328125 \nQ 9.078125 64.109375 15.125 69.15625 \nQ 21.1875 74.21875 31.78125 74.21875 \nQ 42.390625 74.21875 48.4375 69.15625 \nQ 54.5 64.109375 54.5 55.328125 \nQ 54.5 48.6875 50.5625 44.40625 \nQ 46.625 40.140625 39.3125 38.8125 \nz\n\" id=\"DejaVuSerif-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 42.839375 115.797719 \nL 377.639375 115.797719 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\"/>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(21.845938 119.97686)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 42.839375 81.695968 \nL 377.639375 81.695968 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\"/>\n     <g id=\"text_13\">\n      <!-- 1.2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(21.845938 85.875109)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 42.839375 47.594217 \nL 377.639375 47.594217 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\"/>\n     <g id=\"text_14\">\n      <!-- 1.4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(21.845938 51.773358)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#pf5578ca184)\" d=\"M 42.839375 13.492466 \nL 377.639375 13.492466 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\"/>\n     <g id=\"text_15\">\n      <!-- 1.6 -->\n      <g style=\"fill:#262626;\" transform=\"translate(21.845938 17.671606)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSerif-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Loss -->\n     <g style=\"fill:#262626;\" transform=\"translate(15.558281 128.527891)rotate(-90)scale(0.11 -0.11)\">\n      <defs>\n       <path d=\"M 5.515625 0 \nL 5.515625 5.171875 \nL 14.796875 5.171875 \nL 14.796875 67.671875 \nL 5.515625 67.671875 \nL 5.515625 72.90625 \nL 33.984375 72.90625 \nL 33.984375 67.671875 \nL 24.703125 67.671875 \nL 24.703125 6 \nL 58.015625 6 \nL 58.015625 18.21875 \nL 64.015625 18.21875 \nL 64.015625 0 \nz\n\" id=\"DejaVuSerif-76\"/>\n       <path d=\"M 5.609375 2.875 \nL 5.609375 14.984375 \nL 10.796875 14.984375 \nQ 10.984375 9.1875 14.421875 6.296875 \nQ 17.875 3.421875 24.609375 3.421875 \nQ 30.671875 3.421875 33.84375 5.6875 \nQ 37.015625 7.953125 37.015625 12.3125 \nQ 37.015625 15.71875 34.6875 17.8125 \nQ 32.375 19.921875 24.90625 22.3125 \nL 18.40625 24.515625 \nQ 11.71875 26.65625 8.71875 29.875 \nQ 5.71875 33.109375 5.71875 38.09375 \nQ 5.71875 45.21875 10.9375 49.265625 \nQ 16.15625 53.328125 25.390625 53.328125 \nQ 29.5 53.328125 34.03125 52.25 \nQ 38.578125 51.171875 43.40625 49.125 \nL 43.40625 37.796875 \nL 38.234375 37.796875 \nQ 38.03125 42.828125 34.703125 45.65625 \nQ 31.390625 48.484375 25.6875 48.484375 \nQ 20.015625 48.484375 17.109375 46.484375 \nQ 14.203125 44.484375 14.203125 40.484375 \nQ 14.203125 37.203125 16.40625 35.21875 \nQ 18.609375 33.25 25.203125 31.203125 \nL 32.328125 29 \nQ 39.703125 26.703125 42.9375 23.265625 \nQ 46.1875 19.828125 46.1875 14.40625 \nQ 46.1875 7.03125 40.546875 2.796875 \nQ 34.90625 -1.421875 25 -1.421875 \nQ 19.96875 -1.421875 15.1875 -0.34375 \nQ 10.40625 0.734375 5.609375 2.875 \nz\n\" id=\"DejaVuSerif-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-76\"/>\n      <use x=\"66.40625\" xlink:href=\"#DejaVuSerif-111\"/>\n      <use x=\"126.611328\" xlink:href=\"#DejaVuSerif-115\"/>\n      <use x=\"177.929688\" xlink:href=\"#DejaVuSerif-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_29\">\n    <path clip-path=\"url(#pf5578ca184)\" d=\"M 58.057557 17.083636 \nL 63.305206 78.532314 \nL 68.552855 103.116254 \nL 73.800504 112.332481 \nL 79.048152 144.83395 \nL 84.295801 153.491799 \nL 89.54345 153.431952 \nL 94.791099 160.551582 \nL 100.038748 162.696997 \nL 105.286397 170.641796 \nL 110.534046 169.076378 \nL 115.781695 169.90675 \nL 121.029344 179.835354 \nL 126.276993 174.646617 \nL 131.524641 180.61721 \nL 136.77229 181.792357 \nL 142.019939 178.568541 \nL 147.267588 180.172767 \nL 152.515237 183.880755 \nL 157.762886 190.130279 \nL 163.010535 188.568769 \nL 168.258184 192.696213 \nL 173.505833 191.353496 \nL 178.753482 192.733647 \nL 184.00113 192.533701 \nL 189.248779 190.482523 \nL 194.496428 192.077579 \nL 199.744077 191.640188 \nL 204.991726 194.066548 \nL 210.239375 194.205108 \nL 215.487024 192.461151 \nL 220.734673 196.48852 \nL 225.982322 193.777744 \nL 231.229971 194.560813 \nL 236.47762 196.322972 \nL 241.725268 199.712393 \nL 246.972917 193.733635 \nL 252.220566 200.759736 \nL 257.468215 196.829233 \nL 262.715864 202.183113 \nL 267.963513 206.38139 \nL 273.211162 203.918616 \nL 278.458811 201.308984 \nL 283.70646 205.787805 \nL 288.954109 202.826837 \nL 294.201757 202.243873 \nL 299.449406 203.159205 \nL 304.697055 201.279797 \nL 309.944704 202.436206 \nL 315.192353 207.599937 \nL 320.440002 204.371586 \nL 325.687651 207.939267 \nL 330.9353 206.92825 \nL 336.182949 199.728013 \nL 341.430598 209.676601 \nL 346.678246 206.936737 \nL 351.925895 205.05298 \nL 357.173544 206.354529 \nL 362.421193 204.212437 \n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path clip-path=\"url(#pf5578ca184)\" d=\"M 58.057557 79.969913 \nL 63.305206 56.848397 \nL 68.552855 91.14081 \nL 73.800504 174.315032 \nL 79.048152 165.897295 \nL 84.295801 175.867646 \nL 89.54345 154.65225 \nL 94.791099 149.140141 \nL 100.038748 115.105961 \nL 105.286397 171.284326 \nL 110.534046 175.945769 \nL 115.781695 195.646563 \nL 121.029344 192.557739 \nL 126.276993 182.752496 \nL 131.524641 188.816437 \nL 136.77229 183.287919 \nL 142.019939 194.818373 \nL 147.267588 193.232624 \nL 152.515237 188.535915 \nL 157.762886 203.798766 \nL 163.010535 199.869464 \nL 168.258184 180.592941 \nL 173.505833 194.517233 \nL 178.753482 192.391972 \nL 184.00113 180.288763 \nL 189.248779 184.765566 \nL 194.496428 180.396242 \nL 199.744077 189.122507 \nL 204.991726 206.672651 \nL 210.239375 196.967558 \nL 215.487024 201.962144 \nL 220.734673 193.110321 \nL 225.982322 199.082835 \nL 231.229971 193.27768 \nL 236.47762 206.556858 \nL 241.725268 193.084891 \nL 246.972917 212.23746 \nL 252.220566 205.997256 \nL 257.468215 190.726153 \nL 262.715864 208.367734 \nL 267.963513 209.551527 \nL 273.211162 206.269699 \nL 278.458811 194.990182 \nL 283.70646 210.166818 \nL 288.954109 205.654404 \nL 294.201757 209.654532 \nL 299.449406 210.545869 \nL 304.697055 207.573581 \nL 309.944704 209.943052 \nL 315.192353 214.756364 \nL 320.440002 213.379436 \nL 325.687651 206.030367 \nL 330.9353 213.564217 \nL 336.182949 205.552087 \nL 341.430598 207.656784 \nL 346.678246 208.452238 \nL 351.925895 205.892843 \nL 357.173544 203.155167 \nL 362.421193 208.861735 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 42.839375 224.64 \nL 42.839375 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 377.639375 224.64 \nL 377.639375 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 42.839375 224.64 \nL 377.639375 224.64 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 42.839375 7.2 \nL 377.639375 7.2 \n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"line2d_31\">\n     <path d=\"M 260.82625 21.608281 \nL 282.82625 21.608281 \n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_32\"/>\n    <g id=\"text_17\">\n     <!-- Training Loss -->\n     <g style=\"fill:#262626;\" transform=\"translate(291.62625 25.458281)scale(0.11 -0.11)\">\n      <defs>\n       <path d=\"M 19.09375 0 \nL 19.09375 5.171875 \nL 28.421875 5.171875 \nL 28.421875 67.09375 \nL 6.984375 67.09375 \nL 6.984375 55.71875 \nL 0.984375 55.71875 \nL 0.984375 72.90625 \nL 65.71875 72.90625 \nL 65.71875 55.71875 \nL 59.71875 55.71875 \nL 59.71875 67.09375 \nL 38.28125 67.09375 \nL 38.28125 5.171875 \nL 47.609375 5.171875 \nL 47.609375 0 \nz\n\" id=\"DejaVuSerif-84\"/>\n       <path d=\"M 47.796875 52 \nL 47.796875 39.015625 \nL 42.625 39.015625 \nQ 42.390625 42.875 40.484375 44.78125 \nQ 38.578125 46.6875 34.90625 46.6875 \nQ 28.265625 46.6875 24.71875 42.09375 \nQ 21.1875 37.5 21.1875 28.90625 \nL 21.1875 5.171875 \nL 31.59375 5.171875 \nL 31.59375 0 \nL 4.109375 0 \nL 4.109375 5.171875 \nL 12.203125 5.171875 \nL 12.203125 46.78125 \nL 3.609375 46.78125 \nL 3.609375 51.90625 \nL 21.1875 51.90625 \nL 21.1875 42.671875 \nQ 23.828125 48.09375 27.96875 50.703125 \nQ 32.125 53.328125 38.09375 53.328125 \nQ 40.28125 53.328125 42.703125 52.984375 \nQ 45.125 52.640625 47.796875 52 \nz\n\" id=\"DejaVuSerif-114\"/>\n       <path d=\"M 39.796875 16.3125 \nL 39.796875 27.296875 \nL 28.21875 27.296875 \nQ 21.53125 27.296875 18.25 24.40625 \nQ 14.984375 21.53125 14.984375 15.578125 \nQ 14.984375 10.15625 18.296875 6.984375 \nQ 21.625 3.8125 27.296875 3.8125 \nQ 32.90625 3.8125 36.34375 7.28125 \nQ 39.796875 10.75 39.796875 16.3125 \nz\nM 48.78125 32.421875 \nL 48.78125 5.171875 \nL 56.78125 5.171875 \nL 56.78125 0 \nL 39.796875 0 \nL 39.796875 5.609375 \nQ 36.8125 2 32.90625 0.28125 \nQ 29 -1.421875 23.78125 -1.421875 \nQ 15.140625 -1.421875 10.0625 3.171875 \nQ 4.984375 7.765625 4.984375 15.578125 \nQ 4.984375 23.640625 10.796875 28.078125 \nQ 16.609375 32.515625 27.203125 32.515625 \nL 39.796875 32.515625 \nL 39.796875 36.078125 \nQ 39.796875 42 36.203125 45.234375 \nQ 32.625 48.484375 26.125 48.484375 \nQ 20.75 48.484375 17.578125 46.046875 \nQ 14.40625 43.609375 13.625 38.8125 \nL 8.984375 38.8125 \nL 8.984375 49.3125 \nQ 13.671875 51.3125 18.09375 52.3125 \nQ 22.515625 53.328125 26.703125 53.328125 \nQ 37.5 53.328125 43.140625 47.96875 \nQ 48.78125 42.625 48.78125 32.421875 \nz\n\" id=\"DejaVuSerif-97\"/>\n       <path d=\"M 9.71875 68.015625 \nQ 9.71875 70.265625 11.34375 71.921875 \nQ 12.984375 73.578125 15.28125 73.578125 \nQ 17.53125 73.578125 19.15625 71.921875 \nQ 20.796875 70.265625 20.796875 68.015625 \nQ 20.796875 65.71875 19.1875 64.109375 \nQ 17.578125 62.5 15.28125 62.5 \nQ 12.984375 62.5 11.34375 64.109375 \nQ 9.71875 65.71875 9.71875 68.015625 \nz\nM 21.1875 5.171875 \nL 29.6875 5.171875 \nL 29.6875 0 \nL 3.609375 0 \nL 3.609375 5.171875 \nL 12.203125 5.171875 \nL 12.203125 46.6875 \nL 3.609375 46.6875 \nL 3.609375 51.90625 \nL 21.1875 51.90625 \nz\n\" id=\"DejaVuSerif-105\"/>\n       <path d=\"M 4.109375 0 \nL 4.109375 5.171875 \nL 12.203125 5.171875 \nL 12.203125 46.6875 \nL 3.609375 46.6875 \nL 3.609375 51.90625 \nL 21.1875 51.90625 \nL 21.1875 42.671875 \nQ 23.6875 47.953125 27.65625 50.640625 \nQ 31.640625 53.328125 36.921875 53.328125 \nQ 45.515625 53.328125 49.5625 48.390625 \nQ 53.609375 43.453125 53.609375 33.015625 \nL 53.609375 5.171875 \nL 61.625 5.171875 \nL 61.625 0 \nL 36.8125 0 \nL 36.8125 5.171875 \nL 44.578125 5.171875 \nL 44.578125 30.171875 \nQ 44.578125 39.703125 42.234375 43.234375 \nQ 39.890625 46.78125 33.984375 46.78125 \nQ 27.734375 46.78125 24.453125 42.203125 \nQ 21.1875 37.640625 21.1875 28.90625 \nL 21.1875 5.171875 \nL 29 5.171875 \nL 29 0 \nz\n\" id=\"DejaVuSerif-110\"/>\n       <path d=\"M 52.484375 46.6875 \nL 52.484375 1.125 \nQ 52.484375 -10.0625 46.328125 -16.140625 \nQ 40.1875 -22.21875 28.8125 -22.21875 \nQ 23.6875 -22.21875 19 -21.28125 \nQ 14.3125 -20.359375 10.015625 -18.5 \nL 10.015625 -7.625 \nL 14.703125 -7.625 \nQ 15.578125 -12.703125 18.84375 -15.046875 \nQ 22.125 -17.390625 28.21875 -17.390625 \nQ 36.140625 -17.390625 39.8125 -12.921875 \nQ 43.5 -8.453125 43.5 1.125 \nL 43.5 8.109375 \nQ 40.875 3.21875 36.796875 0.890625 \nQ 32.71875 -1.421875 26.703125 -1.421875 \nQ 17.140625 -1.421875 11.0625 6.171875 \nQ 4.984375 13.765625 4.984375 25.984375 \nQ 4.984375 38.1875 11.03125 45.75 \nQ 17.09375 53.328125 26.703125 53.328125 \nQ 32.71875 53.328125 36.796875 51 \nQ 40.875 48.6875 43.5 43.796875 \nL 43.5 51.90625 \nL 61.078125 51.90625 \nL 61.078125 46.6875 \nz\nM 43.5 28.515625 \nQ 43.5 37.84375 39.90625 42.765625 \nQ 36.328125 47.703125 29.5 47.703125 \nQ 22.5625 47.703125 19.015625 42.234375 \nQ 15.484375 36.765625 15.484375 25.984375 \nQ 15.484375 15.234375 19.015625 9.71875 \nQ 22.5625 4.203125 29.5 4.203125 \nQ 36.328125 4.203125 39.90625 9.109375 \nQ 43.5 14.015625 43.5 23.390625 \nz\n\" id=\"DejaVuSerif-103\"/>\n       <path id=\"DejaVuSerif-32\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-84\"/>\n      <use x=\"66.699219\" xlink:href=\"#DejaVuSerif-114\"/>\n      <use x=\"114.501953\" xlink:href=\"#DejaVuSerif-97\"/>\n      <use x=\"174.121094\" xlink:href=\"#DejaVuSerif-105\"/>\n      <use x=\"206.103516\" xlink:href=\"#DejaVuSerif-110\"/>\n      <use x=\"270.507812\" xlink:href=\"#DejaVuSerif-105\"/>\n      <use x=\"302.490234\" xlink:href=\"#DejaVuSerif-110\"/>\n      <use x=\"366.894531\" xlink:href=\"#DejaVuSerif-103\"/>\n      <use x=\"430.908203\" xlink:href=\"#DejaVuSerif-32\"/>\n      <use x=\"462.695312\" xlink:href=\"#DejaVuSerif-76\"/>\n      <use x=\"529.101562\" xlink:href=\"#DejaVuSerif-111\"/>\n      <use x=\"589.306641\" xlink:href=\"#DejaVuSerif-115\"/>\n      <use x=\"640.625\" xlink:href=\"#DejaVuSerif-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_33\">\n     <path d=\"M 260.82625 37.910625 \nL 282.82625 37.910625 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:round;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_34\"/>\n    <g id=\"text_18\">\n     <!-- Test Loss -->\n     <g style=\"fill:#262626;\" transform=\"translate(291.62625 41.760625)scale(0.11 -0.11)\">\n      <defs>\n       <path d=\"M 54.203125 25 \nL 15.484375 25 \nL 15.484375 24.609375 \nQ 15.484375 14.109375 19.4375 8.765625 \nQ 23.390625 3.421875 31.109375 3.421875 \nQ 37.015625 3.421875 40.796875 6.515625 \nQ 44.578125 9.625 46.09375 15.71875 \nL 53.328125 15.71875 \nQ 51.171875 7.171875 45.375 2.875 \nQ 39.59375 -1.421875 30.171875 -1.421875 \nQ 18.796875 -1.421875 11.890625 6.078125 \nQ 4.984375 13.578125 4.984375 25.984375 \nQ 4.984375 38.28125 11.765625 45.796875 \nQ 18.5625 53.328125 29.59375 53.328125 \nQ 41.359375 53.328125 47.65625 46.0625 \nQ 53.953125 38.8125 54.203125 25 \nz\nM 43.609375 30.171875 \nQ 43.3125 39.265625 39.765625 43.875 \nQ 36.234375 48.484375 29.59375 48.484375 \nQ 23.390625 48.484375 19.828125 43.84375 \nQ 16.265625 39.203125 15.484375 30.171875 \nz\n\" id=\"DejaVuSerif-101\"/>\n       <path d=\"M 10.796875 46.6875 \nL 2.875 46.6875 \nL 2.875 51.90625 \nL 10.796875 51.90625 \nL 10.796875 68.015625 \nL 19.828125 68.015625 \nL 19.828125 51.90625 \nL 36.71875 51.90625 \nL 36.71875 46.6875 \nL 19.828125 46.6875 \nL 19.828125 13.71875 \nQ 19.828125 7.125 21.09375 5.265625 \nQ 22.359375 3.421875 25.78125 3.421875 \nQ 29.296875 3.421875 30.90625 5.484375 \nQ 32.515625 7.5625 32.625 12.203125 \nL 39.40625 12.203125 \nQ 39.015625 5.125 35.546875 1.84375 \nQ 32.078125 -1.421875 25 -1.421875 \nQ 17.234375 -1.421875 14.015625 2.015625 \nQ 10.796875 5.46875 10.796875 13.71875 \nz\n\" id=\"DejaVuSerif-116\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-84\"/>\n      <use x=\"58.949219\" xlink:href=\"#DejaVuSerif-101\"/>\n      <use x=\"118.128906\" xlink:href=\"#DejaVuSerif-115\"/>\n      <use x=\"169.447266\" xlink:href=\"#DejaVuSerif-116\"/>\n      <use x=\"209.632812\" xlink:href=\"#DejaVuSerif-32\"/>\n      <use x=\"241.419922\" xlink:href=\"#DejaVuSerif-76\"/>\n      <use x=\"307.826172\" xlink:href=\"#DejaVuSerif-111\"/>\n      <use x=\"368.03125\" xlink:href=\"#DejaVuSerif-115\"/>\n      <use x=\"419.349609\" xlink:href=\"#DejaVuSerif-115\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf5578ca184\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"42.839375\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "matplotlib.rcParams.update({'font.size':11,\n",
    "                            'font.family':'serif'})\n",
    "training_loss = model_trainer.history['loss'][1:]\n",
    "test_loss = model_trainer.history['val_loss'][1:]\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epoch_count, training_loss, 'r--')\n",
    "ax.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "fig.savefig('../plots/loss_curve.pdf')"
   ]
  }
 ]
}